{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "Python 3.8.2 64-bit ('pml')",
      "display_name": "Python 3.8.2 64-bit ('pml')",
      "metadata": {
        "interpreter": {
          "hash": "a4c9474aacc61cf72d0f1c29f4a339e5d6b2171c287541cfd684cf058783219b"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2-final"
    },
    "colab": {
      "name": "013.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "J_f6k7r6MML3"
      },
      "source": [
        "# How to evaluate following systems?\n",
        "\n",
        "## Credit Approval (Yes/No)\n",
        "## Covid detection (Positive/Negative)\n",
        "## Court guilty judgement (Guilty/Not Guilty)\n",
        "## Spam detection (Spam/Ham)\n",
        "## Anomaly detection (Anomaly/Normal)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sApgSj5MML4"
      },
      "source": [
        "# Some classification process under study generated following labels \n",
        "Y =  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "# 2 classification models trying to approximate above process produced following end predictions\n",
        "Y_hat1 = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "Y_hat2 = [1, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n",
        "\n",
        "#TBD Which is the better model\n",
        "# Y_hat1 - less error"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UVQH9_gMML4",
        "outputId": "abd60fff-4758-4ad6-f406-6ebae1a8c2c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TBD: Compute accuracy of the models\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy1 = accuracy_score(Y, Y_hat1)\n",
        "accuracy2 = accuracy_score(Y, Y_hat2)\n",
        "\n",
        "print(f\"accuracy of model1 = {accuracy1}. {accuracy1*100}% accurate\")\n",
        "print(f\"accuracy of model2 = {accuracy2}. {accuracy2*100}% accurate\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy of model1 = 0.9. 90.0% accurate\n",
            "accuracy of model2 = 0.8. 80.0% accurate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTJ-em93MML4",
        "outputId": "2270c973-58f9-4cd6-fa24-47d16675a70a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TBD: Compute confusion matrix for the models\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "Positive/Negative refer to class\n",
        "    1 is Positive Class (Usually rare class should be kept positive as metrics like precision, recall are defined from perspective of positive class)\n",
        "    0 is Negative Class\n",
        "\n",
        "True Positives (TP)  = Classified Positive and are Correct\n",
        "False Positives (FP) = Classified Positive and are Incorrect\n",
        "False Negatives (FN) = Classified Negative and are Incorrect\n",
        "True Negatives (TN)  = Classified Negative and are Correct\n",
        "\n",
        "True/False refers to classification\n",
        "    True means classified correctly\n",
        "    False means classified incorrectly\n",
        "\n",
        "+-------------------------------------------------+\n",
        "|                 Confusion Matrix                |\n",
        "+------------+-----------------+------------------+\n",
        "|            | Actual Positive | Actual Negative  |\n",
        "+------------+-----------------+------------------+\n",
        "| Predicted  | True Positive   | False Positive   |\n",
        "| Positive   |                 |                  |\n",
        "+------------+-----------------+------------------+\n",
        "| Predicted  | False Negative  | True Negative    |\n",
        "| Negative   |                 |                  |\n",
        "+------------+-----------------+------------------+\n",
        "\n",
        "'''\n",
        "# \n",
        "# Y      = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "# Y_hat1 = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "confusion1 = np.array([\n",
        "    [1, 0],\n",
        "    [1, 8]\n",
        "])\n",
        "\n",
        "# \n",
        "# Y      = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "# Y_hat2 = [1, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n",
        "\n",
        "confusion2 = np.array([\n",
        "    [2, 2],\n",
        "    [0, 6]\n",
        "])\n",
        "\n",
        "print(f\"confusion1 = \\n{confusion1}\")\n",
        "print(f\"confusion2 = \\n{confusion2}\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion1 = \n",
            "[[1 0]\n",
            " [1 8]]\n",
            "confusion2 = \n",
            "[[2 2]\n",
            " [0 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU3L_cChMML4",
        "outputId": "20d41dc2-ffa6-49d1-d7cd-c707cf262f41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TBD: Compute precision, recall and f1 score for the models\n",
        "\n",
        "# Precision = TP/(TP + FP)\n",
        "# Precision means what fraction of class we are classifying positive is actually positive\n",
        "precision1 = 1/(1 + 0)\n",
        "print(f\"precision1 = {precision1}\")\n",
        "precision2 = 2/(2 + 2)\n",
        "print(f\"precision2 = {precision2}\")\n",
        "\n",
        "# Recall = TP/(TP + FN)\n",
        "# Recall means what fraction of total positive class we are classifying as positive\n",
        "recall1 = 1/(1+1)\n",
        "print(f\"recall1 = {recall1}\")\n",
        "recall2 = 2/(2+0)\n",
        "print(f\"recall2 = {recall2}\")\n",
        "\n",
        "\n",
        "# Usually ML systems tradeoff between precision and recall, higher precision\n",
        "# may lead to lower recall and vise versa. for e.g in a Serious disease detector\n",
        "# we may favour recall over precision as we did not want to miss any case.\n",
        "# Some judicial systems heavily tilt towards precision at the cost of recall\n",
        "# It doesnt matter if 100 guilty are acquitted, but not one innocent should be punished.\n",
        "# Similarly if designing a anomaly detection system you would favour recall for mission\n",
        "# critical system, but for normal system you might favour precision to avoid waking\n",
        "# engineers at night\n",
        "\n",
        "\n",
        "# F1 score, Usually in case of skewed classes, metrics like accuracy are not\n",
        "# trustworthy, precision, recall serve the purpose but they are 2 metrics\n",
        "# and usually for evaluation of ML systems its reccommended to have single value metrics\n",
        "# F1 combines both precision and recall, its a harmonic mean of precision and recall\n",
        "\n",
        "# f1 = 2 * precision * recall /(precision + recall)\n",
        "\n",
        "def f1(precision, recall):\n",
        "    return 2*precision* recall/(precision + recall)\n",
        "\n",
        "f1_1 = f1(precision1, recall1)\n",
        "print(f\"F1 score of model1 = {f1_1}\")\n",
        "f1_2 = f1(precision2, recall2)\n",
        "print(f\"F1 score of model2 = {f1_2}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision1 = 1.0\n",
            "precision2 = 0.5\n",
            "recall1 = 0.5\n",
            "recall2 = 1.0\n",
            "F1 score of model1 = 0.6666666666666666\n",
            "F1 score of model2 = 0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVorE4BFMML4",
        "outputId": "913c496a-69a2-4412-cd2b-0e78190f58e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision1 = precision_score(Y, Y_hat1) \n",
        "precision2 = precision_score(Y, Y_hat2)\n",
        "\n",
        "print(f\"precision1 = {precision1}\")\n",
        "print(f\"precision2 = {precision2}\")\n",
        "\n",
        "recall1 = recall_score(Y, Y_hat1)\n",
        "recall2 = recall_score(Y, Y_hat2)\n",
        "\n",
        "print(f\"recall1 = {recall1}\")\n",
        "print(f\"recall2 = {recall2}\")\n",
        "\n",
        "f1_1 = f1_score(Y, Y_hat1)\n",
        "f1_2 = f1_score(Y, Y_hat2)\n",
        "\n",
        "print(f\"F1 score of model1 = {f1_1}\")\n",
        "print(f\"F1 score of model2 = {f1_2}\")\n",
        "\n",
        "# Precision, Recall and F1 are defined around notion of positive class\n",
        "# 1 is default positive class, but if you want another label to be positive class\n",
        "# you can override it by specifying pos_label argument\n",
        "\n",
        "model1_precision_wrt_0_as_positive_class = precision_score(Y, Y_hat1, pos_label=0)\n",
        "print(f\"model1 precision with 0 as positive class = {model1_precision_wrt_0_as_positive_class}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision1 = 1.0\n",
            "precision2 = 0.5\n",
            "recall1 = 0.5\n",
            "recall2 = 1.0\n",
            "F1 score of model1 = 0.6666666666666666\n",
            "F1 score of model2 = 0.6666666666666666\n",
            "model1 precision with 0 as positive class = 0.8888888888888888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVibYpu9SUZg",
        "outputId": "cefd8d9a-6f58-4009-f4ba-af9af34b7acb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Note: Confusion matrix using sklearn has Actual Class in Rows and Predicted class in columns (i.e axes are other way round)\n",
        "'''\n",
        "+-----------------------------------------------------+\n",
        "|              Confusion Matrix (sklearn)             |\n",
        "+----------+--------------------+---------------------+\n",
        "|          | Predicted Positive | Predicted Negative  |\n",
        "+----------+--------------------+---------------------+\n",
        "| Actual   | True Positive      | False Negative      |\n",
        "| Positive |                    |                     |\n",
        "+----------+--------------------+---------------------+\n",
        "| Actual   | False Positive     | True Negative       |\n",
        "| Negative |                    |                     |\n",
        "+----------+--------------------+---------------------+\n",
        "'''\n",
        "#Positive and Negative Labels can be specified in labels arguments \n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion1_sklearn = confusion_matrix(Y, Y_hat1, labels = [1, 0]) #Order in which we specify labels will decide which will be positive class, [1, 0] means 1 is positive class, 0 is negative class\n",
        "confusion2_sklearn = confusion_matrix(Y, Y_hat2, labels = [1, 0])\n",
        "print(f\"confusion1_sklearn = \\n{confusion1_sklearn}\")\n",
        "print(f\"confusion2_skelarn = \\n{confusion2_sklearn}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion1_sklearn = \n",
            "[[1 1]\n",
            " [0 8]]\n",
            "confusion2_skelarn = \n",
            "[[2 0]\n",
            " [2 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}