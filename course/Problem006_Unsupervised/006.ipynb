{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "006.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scC6nkOFPzE0"
      },
      "source": [
        "## Unsupervised Learning  - Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DDicYd6cPzE1",
        "outputId": "3e272bfd-c968-4c7f-e2f8-bdc87ff7e0f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "Building a dataset of documents\n",
        "'''\n",
        "\n",
        "'''\n",
        "    Training docs\n",
        "'''\n",
        "doc0 = \"hello hello world \"\n",
        "doc1 = \"hello world world\"\n",
        "doc2 = \"hello world hello world\"\n",
        "doc3 = \"foo foo bar \"\n",
        "doc4 = \"foo bar bar\"\n",
        "doc5 = \"foo bar foo bar\"\n",
        "doc6 = \"lottery prize winner\"\n",
        "doc7 = \"lottery prize\"\n",
        "doc8 = \"lottery lottery lottery prize winner\"\n",
        "docs_train = [doc0, doc1, doc2, doc3, doc4, doc5, doc6, doc7, doc8]\n",
        "\n",
        "'''\n",
        "    Test docs\n",
        "'''\n",
        "doct0 = \"foo bar\"\n",
        "doct1 = \"lottery prize hello\"\n",
        "docs_test = [doct0, doct1]\n",
        "\n",
        "\n",
        "'''\n",
        "    Converting documents to feature vectors\n",
        "'''\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(docs_train).toarray()\n",
        "features = vectorizer.get_feature_names()\n",
        "print(f'Features: {features}')\n",
        "print(f'X_train:\\n====\\n{X_train}\\n====\\n')\n",
        "X_test = vectorizer.transform(docs_test).toarray()\n",
        "print(f'X_test:\\n====\\n{X_test}\\n====\\n')\n",
        "\n",
        "'''\n",
        "Choosing the model\n",
        "'''\n",
        "model = NearestNeighbors(n_neighbors=3, algorithm='brute')\n",
        "\n",
        "'''\n",
        "    Train the model\n",
        "'''\n",
        "model.fit(X_train)\n",
        "\n",
        "'''\n",
        "    Test the model, by getting  k=3 most similar documents, and their distances to the ones in test docs \n",
        "'''\n",
        "distances, indices = model.kneighbors(X_test)\n",
        "\n",
        "print(f'Indices of nearest documents:\\n====\\n{indices}\\n====\\n')\n",
        "print(f'Distances of nearest documents:\\n====\\n{distances}\\n====\\n')\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features: ['bar', 'foo', 'hello', 'lottery', 'prize', 'winner', 'world']\n",
            "X_train:\n",
            "====\n",
            "[[0 0 2 0 0 0 1]\n",
            " [0 0 1 0 0 0 2]\n",
            " [0 0 2 0 0 0 2]\n",
            " [1 2 0 0 0 0 0]\n",
            " [2 1 0 0 0 0 0]\n",
            " [2 2 0 0 0 0 0]\n",
            " [0 0 0 1 1 1 0]\n",
            " [0 0 0 1 1 0 0]\n",
            " [0 0 0 3 1 1 0]]\n",
            "====\n",
            "\n",
            "X_test:\n",
            "====\n",
            "[[1 1 0 0 0 0 0]\n",
            " [0 0 1 1 1 0 0]]\n",
            "====\n",
            "\n",
            "Indices of nearest documents:\n",
            "====\n",
            "[[3 4 5]\n",
            " [7 6 0]]\n",
            "====\n",
            "\n",
            "Distances of nearest documents:\n",
            "====\n",
            "[[1.         1.         1.41421356]\n",
            " [1.         1.41421356 2.        ]]\n",
            "====\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42B8U84OPzE7"
      },
      "source": [
        "## Unsupervised Learning  - Anomaly detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU2h8PXGPzE7",
        "outputId": "7721c7e6-9efb-452b-f27b-ecd4525eeac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\n",
        "\n",
        "'''\n",
        "features = [\"amount_category\", \"merchant_web_mobile\", \"dow\",  \"hour\" ]\n",
        "feature description\n",
        "    amount_category       0 if amount < £100\n",
        "                          1 if  £100 <= amount < £500)\n",
        "                          2 if  £500 <= amount\n",
        "\n",
        "    merchant_web_mobile   0 if  transaction done in shop\n",
        "                          1 if  transaction done through web\n",
        "                          2 if  transaction done through mobile    \n",
        "    \n",
        "    dow                   transaction day of week (1-7)\n",
        "    hour                  transaction hour (1-24)\n",
        "'''\n",
        "def normal_txn_hour():\n",
        "    return random.randint(9,18)\n",
        "def normal_txn_dow():\n",
        "    return random.randint(1,5)\n",
        "\n",
        "'''\n",
        "Building a hypothetical dataset of normal transactions\n",
        "'''\n",
        "N = 1000\n",
        "X_train = [[0, 0, normal_txn_dow(), normal_txn_hour()] for i in range(N)]\n",
        "# print(*X_train, sep = \"\\n\")\n",
        "\n",
        "'''\n",
        "Building a hypothetical test dataset: some fraud transactions and some normal\n",
        "'''\n",
        "X_test = [\n",
        "    [1,2,6,23],\n",
        "    [1,2,6,23],\n",
        "    [0,0,2,11],\n",
        "]\n",
        "\n",
        "'''\n",
        "Choosing a model\n",
        "'''\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "model = EllipticEnvelope()\n",
        "\n",
        "'''\n",
        "    Train the model so that it understand whats normal transaction\n",
        "'''\n",
        "model.fit(X_train)\n",
        "\n",
        "'''\n",
        "   Use the trained model to predict if the given transactions are fraudlent or not\n",
        "'''\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "print(f'Fraud Transaction Prediction (-1 means anomaly/outlier i.e fraud):\\n====\\n{predictions}\\n====\\n')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/covariance/_robust_covariance.py:644: UserWarning: The covariance matrix associated to your dataset is not full rank\n",
            "  warnings.warn(\"The covariance matrix associated to your dataset \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fraud Transaction Prediction (-1 means anomaly/outlier i.e fraud):\n",
            "====\n",
            "[-1 -1  1]\n",
            "====\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Tj7VW5qPzFA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}